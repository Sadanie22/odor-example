{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kDRmPT7DR_-u"
      },
      "outputs": [],
      "source": [
        "# @title Downloads RDkit, Deepchem & Transformers\n",
        "!pip install rdkit-pypi\n",
        "!pip install --pre deepchem\n",
        "!pip install transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbfcpe_BT43l",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import codecs\n",
        "import deepchem\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from collections import Counter\n",
        "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Descriptors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from transformers import BertConfig, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NkRJj3W-UkGU"
      },
      "outputs": [],
      "source": [
        "# @title Check if GPU is available\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c5xsv9v1WQVv"
      },
      "outputs": [],
      "source": [
        "# @title Canonical Smiles Function\n",
        "def get_canonical_smiles(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    return Chem.MolToSmiles(mol, canonical=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtpL5nQVWY0J",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Read in the data and preprocess\n",
        "csv_path = keras.utils.get_file(\n",
        "    \"/content/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        "    \"https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\",\n",
        ")\n",
        "\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "data.rename(columns={'SMILES': 'smiles'}, inplace=True)\n",
        "\n",
        "data = data[data[\"smiles\"].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "data['smiles'] = data['smiles'].apply(get_canonical_smiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klUhI042Zku6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Tokenizer\n",
        "if not os.path.exists('vocab.txt'):\n",
        "    !wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/vocab.txt\n",
        "\n",
        "tokenizer = SmilesTokenizer('vocab.txt')\n",
        "data['tokenized_smiles'] = data['smiles'].apply(tokenizer.encode)\n",
        "data = data[['smiles', 'tokenized_smiles', 'logP', 'qed', 'SAS']]\n",
        "data = data[data['tokenized_smiles'].apply(len) < 50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX0bvxHF-R4v",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Padding\n",
        "def pad_sequence(seq):\n",
        "    return seq + [0] * (50 - len(seq))\n",
        "\n",
        "data['tokenized_smiles'] = data['tokenized_smiles'].apply(pad_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6prTZKgPZtyJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Add descriptors to data and normalize them\n",
        "# try with both selected descriptors and 124 of them to see which one works better\n",
        "selected_descriptors = [\n",
        "        'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3',\n",
        "        'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8',\n",
        "        'EState_VSA9', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12',\n",
        "        'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5',\n",
        "        'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10',\n",
        "        'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8',\n",
        "        'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2',\n",
        "        'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8',\n",
        "        'SlogP_VSA9', 'TPSA'\n",
        "    ]\n",
        "\n",
        "def compute_all_descriptors(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    descriptor_names = [x[0] for x in Descriptors._descList[:124]]\n",
        "    descriptor_values = {}\n",
        "    for name in descriptor_names:\n",
        "        descriptor_func = getattr(Descriptors, name)\n",
        "        descriptor_values[name] = descriptor_func(mol)\n",
        "    return descriptor_values\n",
        "\n",
        "descriptors_df = data['smiles'].apply(compute_all_descriptors).apply(pd.Series)\n",
        "data = pd.concat([data, descriptors_df], axis=1)\n",
        "normalized_data = data.loc[:, 'logP':].apply(lambda x: (x-x.mean()) / x.std(), axis=0)\n",
        "normalized_data_merged = pd.merge(data[['smiles', 'tokenized_smiles']], normalized_data, right_index=True, left_index=True)\n",
        "normalized_data_merged.dropna(axis=0, inplace=True)\n",
        "descriptor_names = normalized_data_merged.columns.tolist()[2:]\n",
        "descriptor_names = list(set(descriptor_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kLeebzszW-h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data['tokenized_smiles'].to_numpy().tolist(), dtype=torch.long)\n",
        "        self.descriptors = torch.tensor(data[descriptor_names].to_numpy(), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = self.data[index]\n",
        "        attention_mask = (input_ids != 0).long()\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'descriptors': self.descriptors[index]\n",
        "        }\n",
        "\n",
        "\n",
        "train_data, temp_data = train_test_split(normalized_data_merged, test_size=0.8, random_state=42)\n",
        "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = Dataset(train_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "validation_dataset = Dataset(validation_data)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "test_dataset = Dataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DYIJBZNim2D",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define BERT model\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig(\n",
        "  vocab_size=len(tokenizer.vocab),\n",
        "  hidden_size=768,\n",
        "  num_hidden_layers=12,\n",
        "  num_attention_heads=12,\n",
        "  intermediate_size=3072,\n",
        ")\n",
        "\n",
        "bert_model = BertModel(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVJPO_jzwzm_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Train MLM\n",
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model = BertForMaskedLM(config=config).to('cuda')\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=18,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./results/bert_base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fK-PjQx1-42",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Test on example prediction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFGbuS7Mz3w_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Multiheaded regression class\n",
        "class DescriptorHead(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=64, output_dim=1):\n",
        "        super(DescriptorHead, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class BertForDescriptors(nn.Module):\n",
        "    def __init__(self, num_descriptors=124):\n",
        "        super(BertForDescriptors, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"./results/bert_base\")\n",
        "        self.descriptor_heads = nn.ModuleList([DescriptorHead() for _ in range(num_descriptors)])\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        descriptor_outputs = []\n",
        "\n",
        "        for head in self.descriptor_heads:\n",
        "            out = head(last_hidden_state_cls)\n",
        "            descriptor_outputs.append(out)\n",
        "\n",
        "        descriptor_outputs = torch.cat(descriptor_outputs, dim=1)\n",
        "\n",
        "        return descriptor_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tg5jWvG1TGe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Initialize model\n",
        "from torch.optim import Adam\n",
        "\n",
        "model = BertForDescriptors().to('cuda')\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss(reduction='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmpwalN91Q6B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Train model\n",
        "# Training loop\n",
        "for epoch in range(12):  # Number of epochs\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to('cuda')\n",
        "        attention_mask = batch['attention_mask'].to('cuda')\n",
        "        labels = batch['descriptors'].to('cuda')\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_loader:\n",
        "            input_ids = batch['input_ids'].to('cuda')\n",
        "            attention_mask = batch['attention_mask'].to('cuda')\n",
        "            labels = batch['descriptors'].to('cuda')\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average validation loss\n",
        "    avg_val_loss = val_loss / len(validation_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "model.bert.save_pretrained(\"./results/bert_desc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czVZyu6s-IYa"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfzDf9rSFPmA"
      },
      "outputs": [],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xpVzhoJ0NgRN"
      },
      "outputs": [],
      "source": [
        "# @title Preprocessing\n",
        "def complete_preprocess(smiles, maxlen=50):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        canonical_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
        "\n",
        "        tokenized = tokenizer.encode(canonical_smiles)\n",
        "\n",
        "        if len(tokenized) > maxlen:\n",
        "            return None\n",
        "\n",
        "        padded = pad_sequence(tokenized)\n",
        "\n",
        "        return padded\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEJUgCnNQ4bT"
      },
      "outputs": [],
      "source": [
        "loaded_bert = BertModel.from_pretrained(\"./results/bert_desc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kub4QUNKTy2Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get Fingerprint\n",
        "def get_representation(smiles, model):\n",
        "    try:\n",
        "        preprocessed_data = complete_preprocess(smiles)\n",
        "        if preprocessed_data is None:\n",
        "            return None\n",
        "\n",
        "        padded_sequence = torch.tensor([preprocessed_data])\n",
        "\n",
        "        attention_mask = (torch.tensor(padded_sequence) != 0).long()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(padded_sequence, attention_mask)\n",
        "\n",
        "        # Extract the [CLS] token's features\n",
        "        cls_features = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return cls_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kL4nShQUY8D"
      },
      "outputs": [],
      "source": [
        "get_representation('COC', loaded_bert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B4OpiiagXbX"
      },
      "outputs": [],
      "source": [
        "csv_file_path = \"your-data.csv\"\n",
        "data_toxic = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZnkNUX-g4Dv"
      },
      "outputs": [],
      "source": [
        "for i in range(768):\n",
        "    data_toxic[f'dim_{i+1}'] = None\n",
        "\n",
        "# Loop through each SMILES string\n",
        "for index, row in data_toxic.iterrows():\n",
        "    smiles = row['SMILES']\n",
        "    representation = get_representation(smiles, loaded_bert)\n",
        "\n",
        "    if representation is not None:\n",
        "        for i, value in enumerate(representation[0]):\n",
        "            data_toxic.at[index, f'dim_{i+1}'] = value.item()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}